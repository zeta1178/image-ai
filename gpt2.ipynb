{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name michaelcruz@aim.com to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::193309394638:role/SageMakerRole\n",
      "sagemaker bucket: sagemaker-us-east-1-193309394638\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# role = get_execution_role()\n",
    "dev=boto3.session.Session()\n",
    "region=dev.region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'gpt-serverless-model'\n",
    "sm_client = dev.client(\"sagemaker\")\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam_client = dev.client('iam')\n",
    "    role = iam_client.get_role(RoleName='SageMakerRole')['Role']['Arn']\n",
    "\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")\n",
    "\n",
    "aws_role = role\n",
    "aws_region = region\n",
    "sess = sagemaker_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/vocab.json', 'model/merges.txt')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "model_path = 'model/'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    \n",
    "model.save_pretrained(save_directory=model_path)\n",
    "tokenizer.save_vocabulary(save_directory=model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir model/code\n",
    "!cp code/inference.py model/code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mos\u001b[39;00m\n",
      "\u001b[38;2;0;128;0;01mimport\u001b[39;00m \u001b[38;2;0;0;255;01mjson\u001b[39;00m\n",
      "\u001b[38;2;0;128;0;01mfrom\u001b[39;00m \u001b[38;2;0;0;255;01mtransformers\u001b[39;00m \u001b[38;2;0;128;0;01mimport\u001b[39;00m GPT2Tokenizer, TextGenerationPipeline, GPT2LMHeadModel\n",
      "\n",
      "\u001b[38;2;61;123;123;03m# Load the model for inference\u001b[39;00m\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mmodel_fn\u001b[39m(model_dir):\n",
      "\n",
      "    \u001b[38;2;61;123;123;03m# Load GPT2 tokenizer from disk.\u001b[39;00m\n",
      "    vocab_path \u001b[38;2;102;102;102m=\u001b[39m os\u001b[38;2;102;102;102m.\u001b[39mpath\u001b[38;2;102;102;102m.\u001b[39mjoin(model_dir, \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mvocab.json\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\n",
      "    merges_path \u001b[38;2;102;102;102m=\u001b[39m os\u001b[38;2;102;102;102m.\u001b[39mpath\u001b[38;2;102;102;102m.\u001b[39mjoin(model_dir, \u001b[38;2;186;33;33m'\u001b[39m\u001b[38;2;186;33;33mmerges.txt\u001b[39m\u001b[38;2;186;33;33m'\u001b[39m)\n",
      "    \n",
      "    tokenizer \u001b[38;2;102;102;102m=\u001b[39m GPT2Tokenizer(vocab_file\u001b[38;2;102;102;102m=\u001b[39mvocab_path, merges_file\u001b[38;2;102;102;102m=\u001b[39mmerges_path)\n",
      "\n",
      "    \u001b[38;2;61;123;123;03m# Load GPT2 model from disk.\u001b[39;00m\n",
      "    model \u001b[38;2;102;102;102m=\u001b[39m GPT2LMHeadModel\u001b[38;2;102;102;102m.\u001b[39mfrom_pretrained(model_dir)\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m TextGenerationPipeline(model\u001b[38;2;102;102;102m=\u001b[39mmodel, tokenizer\u001b[38;2;102;102;102m=\u001b[39mtokenizer)\n",
      "\n",
      "\u001b[38;2;61;123;123;03m# Apply model to the incoming request\u001b[39;00m\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255mpredict_fn\u001b[39m(input_data, model):\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m model\u001b[38;2;102;102;102m.\u001b[39m\u001b[38;2;0;0;255m__call__\u001b[39m(input_data)\n",
      "\n",
      "\u001b[38;2;61;123;123;03m# Deserialize and prepare the prediction input\u001b[39;00m\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255minput_fn\u001b[39m(request_body, request_content_type):\n",
      "\n",
      "    \u001b[38;2;0;128;0;01mif\u001b[39;00m request_content_type \u001b[38;2;102;102;102m==\u001b[39m \u001b[38;2;186;33;33m\"\u001b[39m\u001b[38;2;186;33;33mapplication/json\u001b[39m\u001b[38;2;186;33;33m\"\u001b[39m:\n",
      "        request \u001b[38;2;102;102;102m=\u001b[39m json\u001b[38;2;102;102;102m.\u001b[39mloads(request_body)\n",
      "    \u001b[38;2;0;128;0;01melse\u001b[39;00m:\n",
      "        request \u001b[38;2;102;102;102m=\u001b[39m request_body\n",
      "\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m request\n",
      "\n",
      "\u001b[38;2;61;123;123;03m# Serialize and prepare the prediction output\u001b[39;00m\n",
      "\u001b[38;2;0;128;0;01mdef\u001b[39;00m \u001b[38;2;0;0;255moutput_fn\u001b[39m(prediction, response_content_type):\n",
      "    \u001b[38;2;0;128;0;01mreturn\u001b[39;00m \u001b[38;2;0;128;0mstr\u001b[39m(prediction)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize model/code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a .\n",
      "a ./config.json\n",
      "a ./code\n",
      "a ./merges.txt\n",
      "a ./model.tar.gztar: ./model.tar.gz: Can't add archive to itself\n",
      "\n",
      "a ./pytorch_model.bin\n",
      "a ./vocab.json\n",
      "a ./code/inference.py\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf model/model.tar.gz -C model/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-193309394638/gpt-serverless-model/model.tar.gz'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "model_data = S3Uploader.upload('model/model.tar.gz', 's3://{0}/{1}'.format(bucket,prefix))\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-cpu-py38-ubuntu20.04\"\n",
    "\n",
    "model_name    = 'gpt-2-serverless-model'\n",
    "epc_name     = 'gpt-2-serverless-model-epc'\n",
    "endpoint_name = 'gpt-2-serverless-model-ep'\n",
    "\n",
    "primary_container = {\n",
    "    'Image': image_uri,\n",
    "    'ModelDataUrl': model_data,\n",
    "    'Environment': {\n",
    "        'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "        'SAGEMAKER_REGION': region,\n",
    "        'SAGEMAKER_SUBMIT_DIRECTORY': model_data\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create/Register a GPT-2 model in SM\n",
    "# from sagemaker import get_execution_role\n",
    "\n",
    "# create_model_response = sm_client.create_model(ModelName = model_name,\n",
    "#                                             #   ExecutionRoleArn = get_execution_role(),\n",
    "#                                               ExecutionRoleArn = role,\n",
    "#                                               PrimaryContainer = primary_container)\n",
    "\n",
    "# print(create_model_response['ModelArn'])\n",
    "\n",
    "# # Create a SM Serverless endpoint config\n",
    "# endpoint_config_response = sm_client.create_endpoint_config(\n",
    "#     EndpointConfigName = epc_name,\n",
    "#     ProductionVariants=[\n",
    "#         {\n",
    "#         'ServerlessConfig':{\n",
    "#             'MemorySizeInMB' : 6144,\n",
    "#             'MaxConcurrency' : 5\n",
    "#         },\n",
    "#         'ModelName':model_name,\n",
    "#         'VariantName':'AllTraffic',\n",
    "#         'InitialVariantWeight':1\n",
    "#         }\n",
    "#     ])\n",
    "\n",
    "# print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))\n",
    "\n",
    "# # Create a SM Serverless endpoint config\n",
    "# endpoint_params = {\n",
    "#     'EndpointName': endpoint_name,\n",
    "#     'EndpointConfigName': epc_name,\n",
    "# }\n",
    "# endpoint_response = sm_client.create_endpoint(EndpointName=endpoint_name, EndpointConfigName=epc_name)\n",
    "# print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\\'generated_text\\': \\'\"Working with motorcyles is \"a great way to get the most out of your bike,\"\\'}]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "invoke_client = boto3.client('sagemaker-runtime')\n",
    "prompt = \"Working with motorcyles is \"\n",
    "    \n",
    "response = invoke_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                            Body=json.dumps(prompt),\n",
    "                            ContentType='text/csv')\n",
    "\n",
    "response['Body'].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm_client.delete_model(ModelName=model_name)\n",
    "# sm_client.delete_endpoint_config(EndpointConfigName=epc_name)\n",
    "# sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
